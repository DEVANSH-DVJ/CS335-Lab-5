{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "### Preprocessing and Normalizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    \"\"\"\n",
    "    Implement Normalization for input image features\n",
    "\n",
    "    Args:\n",
    "        X: input features - numpy array of shape (n_samples, 784)\n",
    "\n",
    "    Returns:\n",
    "        X_out: normalized features - numpy array of shape (n_samples, 784)\n",
    "    \"\"\"\n",
    "\n",
    "    return X.astype(np.float32) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def normalizing(X, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Implement Normalization for input features\n",
    "\n",
    "    Args:\n",
    "        X: input features - numpy array of shape (n_samples, 2048)\n",
    "\n",
    "    Returns:\n",
    "        X_out: normalized features - numpy array of shape (n_samples, 2048)\n",
    "    \"\"\"\n",
    "\n",
    "    if mean is None or std is None:\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "\n",
    "    return (X - mean) / std, mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    '''\n",
    "    Split data into train and validation sets\n",
    "    floor(train_ratio*n_samples) samples form the train set and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "        X: data - numpy array of shape (n_samples, n_features)\n",
    "        Y: labels - numpy array of shape (n_samples, 1)\n",
    "        train_ratio: fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "        X_train: train data - numpy array of shape (floor(train_ratio*n_samples), n_features)\n",
    "        Y_train: train labels - numpy array of shape (floor(train_ratio*n_samples), 1)\n",
    "        X_val: test data - numpy array of shape (n_samples - floor(train_ratio*n_samples), n_features)\n",
    "        Y_val: test labels - numpy array of shape (n_samples - floor(train_ratio*n_samples), 1)\n",
    "    '''\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "    X_val, Y_val = X[val_indices], Y[val_indices]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FujjCCbMbsu4"
   },
   "source": [
    "### Flatten the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl8LxP1lAEiN"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    \"\"\"\n",
    "    This class converts a multi-dimensional into 1-d vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape : Original shape, tuple of ints\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.new_shape = np.prod(input_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Converts a multi-dimensional into 1-d vector\n",
    "\n",
    "        Args:\n",
    "            input: training data, numpy array of shape (n_samples, self.input_shape)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, self.new_shape)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.reshape(input, (input.shape[0], self.new_shape))\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Converts back the passed array to original dimension\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (1, self.new_shape)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            output_error: numpy array of shape (1, self.input_shape)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.reshape(output_error, (output_error.shape[0], *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02MOHEdgh7T6"
   },
   "source": [
    "### Fully Connected Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTrTMpTwtLXd"
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    \"\"\"\n",
    "    Implements a fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Input shape, int\n",
    "            output_size: Output shape, int\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Random initialization of weights and bias\n",
    "        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)\n",
    "        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of a fully connected network\n",
    "\n",
    "        Args:\n",
    "            input: training data, numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, self.output_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input for calculating d_weights in backward pass\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (n_samples, self.output_size)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            input_error: numpy array of shape (n_samples, self.input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute gradients\n",
    "        d_weights = np.dot(self.input.T, output_error) # del E / del W\n",
    "        d_bias = np.sum(output_error, axis=0) # del E / del B\n",
    "        input_error = np.dot(output_error, self.weights.T) # del E / del X\n",
    "\n",
    "        # Update weights\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "\n",
    "        # Return error for previous layer\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6nSYAB2sam3"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    \"\"\"\n",
    "    Implements a Activation layer which applies activation function on the inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            activation : activation function (sigmoid, tanh or relu)\n",
    "            activation_prime: derivative of activation function (sigmoid_prime,tanh_prime or relu_prime)\n",
    "        \"\"\"\n",
    "\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies the activation function\n",
    "\n",
    "        Args:\n",
    "            input: numpy array of shape (n_samples, input_size)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input for calculating input_error in backward pass\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (n_samples, input_size)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            input_error: numpy array of shape (n_samples, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        return output_error * self.activation_prime(self.input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQeuIfkK3vyl"
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    \"\"\"\n",
    "    Implements a Softmax layer which applies softmax function on the inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Input shape, int\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies the softmax function\n",
    "\n",
    "        Args:\n",
    "            input: numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, self.input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "        self.output = np.exp(input) / np.sum(np.exp(input), axis=1).reshape((input.shape[0], 1))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backward pass of a Softmax layer\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (n_samples, self.input_size)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            input_error: numpy array of shape (n_samples, self.input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        input_error = np.zeros_like(output_error)\n",
    "        for i in range(output_error.shape[0]):\n",
    "            out = np.tile(self.output[i].T, (self.input_size, 1))\n",
    "            input_error[i] = self.output[i].reshape((1, self.input_size)) * np.dot(output_error[i], np.identity(self.input_size) - out)\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuPbn70Wt8Q7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        sig(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    Derivative of Sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        sig'(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        tanh(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_prime(x):\n",
    "    \"\"\"\n",
    "    Derivative of Tanh function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        tanh'(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        relu(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def relu_prime(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        relu'(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array(x >= 0).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXY7jkUzuqEk"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MSE loss\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels - numpy array of shape (n_samples, self.input_size)\n",
    "        y_pred: Predicted labels - numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "    Returns:\n",
    "        loss : float\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sum(np.mean((y_true - y_pred)**2, axis=0))\n",
    "\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of MSE loss\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels - numpy array of shape (n_samples, self.input_size)\n",
    "        y_pred: Predicted labels - numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "    Returns:\n",
    "        derivatives: numpy array of shape (n_samples, self.input_size)\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 * (y_pred - y_true) / y_pred.shape[1]\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Cross entropy loss\n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array\n",
    "        y_true :  Predicted labels, numpy array\n",
    "    Returns:\n",
    "       loss : float\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO\n",
    "\n",
    "    # Modify the return statement to return numpy array resulting from backward pass\n",
    "    return 0\n",
    "    ## END TODO\n",
    "\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Implements derivative of cross entropy function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array\n",
    "    Returns:\n",
    "        Numpy array after applying derivative of cross entropy function\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO\n",
    "\n",
    "    # Modify the return statement to return numpy array resulting from backward pass\n",
    "    return None\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23euUDztNtb"
   },
   "source": [
    "### Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sCYdGN8tSdp"
   },
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train, dataset_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Create and trains a feedforward network\n",
    "\n",
    "    Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function\n",
    "    Args:\n",
    "        X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        dataset_name -- name of the dataset (flowers or mnist)\n",
    "    \"\"\"\n",
    "\n",
    "    # Note that this just a template to help you create your own feed forward network\n",
    "    ## TODO\n",
    "\n",
    "    # define your network\n",
    "    # This network would work only for mnist\n",
    "    network = [\n",
    "        FlattenLayer(input_shape=(28, 28)),\n",
    "        FCLayer(28 * 28, 12),\n",
    "        ActivationLayer(sigmoid, sigmoid_prime),\n",
    "        FCLayer(12, 10),\n",
    "        SoftmaxLayer(10),\n",
    "    ]  # This creates feed forward\n",
    "\n",
    "    # Choose appropriate learning rate and no. of epoch\n",
    "    epochs = 40\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    # Change training loop as you see fit\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for x, y_true in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = x\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            # error (display purpose only)\n",
    "            error += mse(y_true, output)\n",
    "\n",
    "            # backward\n",
    "            output_error = mse_prime(y_true, output)\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        print(\"%d/%d, error=%f\" % (epoch + 1, epochs, error))\n",
    "\n",
    "    # Save you model weights\n",
    "\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Pop_HsvuEZ"
   },
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttYbN2psvtu_"
   },
   "outputs": [],
   "source": [
    "dataset = \"mnist\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "fit(train_mnist[0], train_mnist[1], \"mnist\")\n",
    "\n",
    "dataset = \"flowers\"  # \"mnist\"/\"flowers\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_flowers = pkl.load(file)\n",
    "    print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "fit(train_flowers[0], train_flowers[1], \"flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, dataset_name):\n",
    "    \"\"\"\n",
    "    X_test -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "\n",
    "    This is the only function that we will call from the auto grader. \n",
    "\n",
    "    This function should only perform inference, please donot train your models here.\n",
    "\n",
    "    Steps to be done here:\n",
    "    1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
    "    2. Ensure that you read weights using only the libraries we have given above.\n",
    "    3. Initialize your model with your trained weights\n",
    "    4. Compute the predicted labels and return it\n",
    "\n",
    "    Please provide us the complete code you used for training including any techniques\n",
    "    like data augmentation etc. that you have tried out. \n",
    "\n",
    "    Return:\n",
    "        Y_test - nparray of shape (num_test,)\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_test = np.zeros(X_test.shape,)\n",
    "\n",
    "    ## TODO\n",
    "\n",
    "    ## END TODO\n",
    "    assert Y_test.shape == (X_test.shape,) and type(Y_test) == type(X_test), \"Check what you return\"\n",
    "    return Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
