{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pickle as pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "### Preprocessing and Normalizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    \"\"\"\n",
    "    Implement Normalization for input image features\n",
    "\n",
    "    Args:\n",
    "        X: input features - numpy array of shape (n_samples, 784)\n",
    "\n",
    "    Returns:\n",
    "        X_out: normalized features - numpy array of shape (n_samples, 784)\n",
    "    \"\"\"\n",
    "\n",
    "    return X.astype(np.float32) / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def normalizing(X, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Implement Normalization for input features\n",
    "\n",
    "    Args:\n",
    "        X: input features - numpy array of shape (n_samples, 2048)\n",
    "\n",
    "    Returns:\n",
    "        X_out: normalized features - numpy array of shape (n_samples, 2048)\n",
    "    \"\"\"\n",
    "\n",
    "    if mean is None or std is None:\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "\n",
    "    return (X - mean) / std, mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    '''\n",
    "    Split data into train and validation sets\n",
    "    floor(train_ratio*n_samples) samples form the train set and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "        X: data - numpy array of shape (n_samples, n_features)\n",
    "        Y: labels - numpy array of shape (n_samples, 1)\n",
    "        train_ratio: fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "        X_train: train data - numpy array of shape (floor(train_ratio*n_samples), n_features)\n",
    "        Y_train: train labels - numpy array of shape (floor(train_ratio*n_samples), 1)\n",
    "        X_val: test data - numpy array of shape (n_samples - floor(train_ratio*n_samples), n_features)\n",
    "        Y_val: test labels - numpy array of shape (n_samples - floor(train_ratio*n_samples), 1)\n",
    "    '''\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "    X_val, Y_val = X[val_indices], Y[val_indices]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FujjCCbMbsu4"
   },
   "source": [
    "### Flatten the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl8LxP1lAEiN"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    \"\"\"\n",
    "    This class converts a multi-dimensional into 1-d vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape : Original shape, tuple of ints\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.new_shape = np.prod(input_shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Converts a multi-dimensional into 1-d vector\n",
    "\n",
    "        Args:\n",
    "            input: training data, numpy array of shape (n_samples, self.input_shape)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, self.new_shape)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.reshape(input, (input.shape[0], self.new_shape))\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Converts back the passed array to original dimension\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (1, self.new_shape)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            output_error: numpy array of shape (1, self.input_shape)\n",
    "        \"\"\"\n",
    "\n",
    "        return np.reshape(output_error, (output_error.shape[0], *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02MOHEdgh7T6"
   },
   "source": [
    "### Fully Connected Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTrTMpTwtLXd"
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    \"\"\"\n",
    "    Implements a fully connected layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Input shape, int\n",
    "            output_size: Output shape, int\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Random initialization of weights and bias\n",
    "        self.weights = np.random.randn(input_size, output_size) / np.sqrt(input_size + output_size)\n",
    "        self.bias = np.random.randn(1, output_size) / np.sqrt(input_size + output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of a fully connected network\n",
    "\n",
    "        Args:\n",
    "            input: training data, numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, self.output_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input for calculating d_weights in backward pass\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (n_samples, self.output_size)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            input_error: numpy array of shape (n_samples, self.input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute gradients\n",
    "        d_weights = np.dot(self.input.T, output_error) # del E / del W\n",
    "        d_bias = np.sum(output_error, axis=0) # del E / del B\n",
    "        input_error = np.dot(output_error, self.weights.T) # del E / del X\n",
    "\n",
    "        # Update weights\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "\n",
    "        # Return error for previous layer\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6nSYAB2sam3"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    \"\"\"\n",
    "    Implements a Activation layer which applies activation function on the inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            activation : activation function (sigmoid, tanh or relu)\n",
    "            activation_prime: derivative of activation function (sigmoid_prime,tanh_prime or relu_prime)\n",
    "        \"\"\"\n",
    "\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies the activation function\n",
    "\n",
    "        Args:\n",
    "            input: numpy array of shape (n_samples, input_size)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input for calculating input_error in backward pass\n",
    "        self.input = input\n",
    "        return self.activation(input)\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (n_samples, input_size)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            input_error: numpy array of shape (n_samples, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        return output_error * self.activation_prime(self.input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQeuIfkK3vyl"
   },
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    \"\"\"\n",
    "    Implements a Softmax layer which applies softmax function on the inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Input shape, int\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies the softmax function\n",
    "\n",
    "        Args:\n",
    "            input: numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "        Returns:\n",
    "            output: numpy array of shape (n_samples, self.input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "        self.output = np.exp(input) / np.sum(np.exp(input), axis=1).reshape((input.shape[0], 1))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs a backward pass of a Softmax layer\n",
    "\n",
    "        Args:\n",
    "            output_error: numpy array of shape (n_samples, self.input_size)\n",
    "            learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "            input_error: numpy array of shape (n_samples, self.input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        input_error = np.zeros_like(output_error)\n",
    "        for i in range(output_error.shape[0]):\n",
    "            out = np.tile(self.output[i].T, (self.input_size, 1))\n",
    "            input_error[i] = self.output[i].reshape((1, self.input_size)) * np.dot(output_error[i], np.identity(self.input_size) - out)\n",
    "        return input_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuPbn70Wt8Q7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        sig(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    Derivative of Sigmoid function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        sig'(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.exp(-x) / (1 + np.exp(-x))**2\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Tanh function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        tanh(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_prime(x):\n",
    "    \"\"\"\n",
    "    Derivative of Tanh function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        tanh'(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        relu(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def relu_prime(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function\n",
    "\n",
    "    Args:\n",
    "        x: numpy array\n",
    "\n",
    "    Returns:\n",
    "        relu'(x)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.array(x >= 0).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXY7jkUzuqEk"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    MSE loss\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels - numpy array of shape (n_samples, self.input_size)\n",
    "        y_pred: Predicted labels - numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "    Returns:\n",
    "        loss : float\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sum(np.mean((y_true - y_pred)**2, axis=0))\n",
    "\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of MSE loss\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels - numpy array of shape (n_samples, self.input_size)\n",
    "        y_pred: Predicted labels - numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "    Returns:\n",
    "        derivatives: numpy array of shape (n_samples, self.input_size)\n",
    "    \"\"\"\n",
    "\n",
    "    return 2 * (y_pred - y_true) / y_pred.shape[1]\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Cross entropy loss\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels - numpy array of shape (n_samples, self.input_size)\n",
    "        y_pred: Predicted labels - numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "    Returns:\n",
    "        loss : float\n",
    "    \"\"\"\n",
    "\n",
    "    return -np.sum(np.mean(y_true * np.log(y_pred + 1e-8), axis=0))\n",
    "    # return -np.sum(np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8), axis=0))\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of Cross entropy loss\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth labels - numpy array of shape (n_samples, self.input_size)\n",
    "        y_pred: Predicted labels - numpy array of shape (n_samples, self.input_size)\n",
    "\n",
    "    Returns:\n",
    "        derivatives: numpy array of shape (n_samples, self.input_size)\n",
    "    \"\"\"\n",
    "\n",
    "    return -y_true / (y_pred + 1e-8) / y_true.shape[0]\n",
    "    # return -(y_true / (y_pred + 1e-8) - (1 - y_true) / (1 - y_pred + 1e-8)) / y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23euUDztNtb"
   },
   "source": [
    "### Fit function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sCYdGN8tSdp"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(y, length):\n",
    "    Y = np.zeros((y.shape[0], length))\n",
    "    for i in range(y.shape[0]):\n",
    "        Y[i, y[i]] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "def fit(X_train, Y_train, dataset_name):\n",
    "    \"\"\"\n",
    "    Create and trains a feedforward network\n",
    "    Do not forget to save the final weights of the feed forward network to a file.\n",
    "    Use these weights in the `predict` function\n",
    "\n",
    "    Args:\n",
    "        X_train: numpy array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y_train: numpy array of share (num_test, 1) for flowers and (num_test, 1) for mnist.\n",
    "        dataset_name: name of the dataset ('flowers' or 'mnist')\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "        network = [\n",
    "            FlattenLayer(input_shape=(28, 28)),\n",
    "            FCLayer(28 * 28, 200),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(200, 50),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(50, 10),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            SoftmaxLayer(10)\n",
    "        ]\n",
    "\n",
    "        # Choose appropriate learning rate and no. of epoch\n",
    "        epochs = 20\n",
    "        learning_rate = 0.5\n",
    "        batch_size = 64\n",
    "\n",
    "        x_train = preprocessing(X_train)\n",
    "        y_train = encode_onehot(Y_train, 10)\n",
    "\n",
    "    elif dataset_name == 'flowers':\n",
    "        network = [\n",
    "            FCLayer(2048, 512),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(512, 128),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(128, 32),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(32, 5),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            SoftmaxLayer(5)\n",
    "        ]\n",
    "\n",
    "        # Choose appropriate learning rate and no. of epoch\n",
    "        epochs = 12\n",
    "        learning_rate = 0.01\n",
    "        batch_size = 8\n",
    "\n",
    "        x_train, mean, std = normalizing(X_train)\n",
    "        y_train = encode_onehot(Y_train, 5)\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Change training loop as you see fit\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for batch_start in range(0, len(x_train) - 1, batch_size):\n",
    "            x = x_train[batch_start:batch_start+batch_size]\n",
    "            y = y_train[batch_start:batch_start+batch_size]\n",
    "\n",
    "            # forward\n",
    "            output = x\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            # error (display purpose only)\n",
    "            error += cross_entropy(y, output) * len(x)\n",
    "\n",
    "            # backward\n",
    "            output_error = cross_entropy_prime(y, output)\n",
    "\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        print(\"%d/%d, error=%f\" % (epoch + 1, epochs, error))\n",
    "\n",
    "    # Save the model weights\n",
    "    wbs = []\n",
    "    for layer in network:\n",
    "        if isinstance(layer, FCLayer):\n",
    "            wbs.append([layer.weights, layer.bias])\n",
    "\n",
    "    if dataset_name == 'flowers':\n",
    "        wbs.append([mean, std])\n",
    "\n",
    "    with open(f\"./models/{dataset_name}_weights.pkl\", \"wb\") as file:\n",
    "        pkl.dump(wbs, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def decode_onehot(y):\n",
    "    return np.argmax(y, axis=1)\n",
    "\n",
    "\n",
    "def predict(X_test, dataset_name):\n",
    "    \"\"\"\n",
    "    This is the only function that we will call from the auto grader.\n",
    "\n",
    "    This function should only perform inference, please do not train your models here.\n",
    "\n",
    "    Steps to be done here:\n",
    "    1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
    "    2. Ensure that you read weights using only the libraries we have given above.\n",
    "    3. Initialize your model with your trained weights\n",
    "    4. Compute the predicted labels and return it\n",
    "\n",
    "    Please provide us the complete code you used for training including any techniques\n",
    "    like data augmentation etc. that you have tried out.\n",
    "\n",
    "    Args:\n",
    "        X_test: np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist\n",
    "\n",
    "    Return:\n",
    "        Y_test - numpy array of shape (num_test,)\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "        network = [\n",
    "            FlattenLayer(input_shape=(28, 28)),\n",
    "            FCLayer(28 * 28, 200),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(200, 50),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(50, 10),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            SoftmaxLayer(10)\n",
    "        ]\n",
    "\n",
    "    elif dataset_name == 'flowers':\n",
    "        network = [\n",
    "            FCLayer(2048, 512),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(512, 128),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(128, 32),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(32, 5),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            SoftmaxLayer(5)\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Load the model weights\n",
    "    with open(f\"./models/{dataset_name}_weights.pkl\", \"rb\") as file:\n",
    "        wbs = pkl.load(file)\n",
    "\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        if isinstance(layer, FCLayer):\n",
    "            layer.weights = wbs[i][0]\n",
    "            layer.bias = wbs[i][1]\n",
    "            i += 1\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "        x_test = preprocessing(X_test)\n",
    "    elif dataset_name == 'flowers':\n",
    "        mean = wbs[i][0]\n",
    "        std = wbs[i][1]\n",
    "        x_test, mean, std = normalizing(X_test, mean, std)\n",
    "\n",
    "    output = x_test\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    Y_test = decode_onehot(output)\n",
    "\n",
    "    assert Y_test.shape == (X_test.shape[0],) and type(Y_test) == type(X_test), \"Check what you return\"\n",
    "    return Y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Pop_HsvuEZ"
   },
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttYbN2psvtu_"
   },
   "outputs": [],
   "source": [
    "dataset = \"mnist\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "fit(train_mnist[0], train_mnist[1], \"mnist\")\n",
    "\n",
    "dataset = \"flowers\"  # \"mnist\"/\"flowers\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_flowers = pkl.load(file)\n",
    "    print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "fit(train_flowers[0], train_flowers[1], \"flowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def train_validate(X, Y, dataset_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Create, train and validate a feedforward network\n",
    "\n",
    "    Args:\n",
    "        X -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y -- np array of share (num_test, 1) for flowers and (num_test, 1) for mnist.\n",
    "        dataset_name -- name of the dataset ('flowers' or 'mnist')\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "        network = [\n",
    "            FlattenLayer(input_shape=(28, 28)),\n",
    "            FCLayer(28 * 28, 200),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(200, 50),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(50, 10),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            SoftmaxLayer(10)\n",
    "        ]\n",
    "\n",
    "        X_train, Y_train, X_val, Y_val = split_data(X, Y, train_ratio=0.8)\n",
    "        x_train = preprocessing(X_train)\n",
    "        y_train = encode_onehot(Y_train, 10)\n",
    "        x_val = preprocessing(X_val)\n",
    "        y_val = encode_onehot(Y_val, 10)\n",
    "\n",
    "        # Choose appropriate learning rate and no. of epoch\n",
    "        epochs = 40\n",
    "        learning_rate = 0.5\n",
    "        batch_size = 64\n",
    "\n",
    "    elif dataset_name == 'flowers':\n",
    "        network = [\n",
    "            FCLayer(2048, 512),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(512, 128),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(128, 32),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            FCLayer(32, 5),\n",
    "            ActivationLayer(tanh, tanh_prime),\n",
    "            SoftmaxLayer(5)\n",
    "        ]\n",
    "\n",
    "        X_train, Y_train, X_val, Y_val = split_data(X, Y, train_ratio=0.8)\n",
    "        x_train, mean, std = normalizing(X_train)\n",
    "        y_train = encode_onehot(Y_train, 5)\n",
    "        x_val, mean, std = normalizing(X_val, mean, std)\n",
    "        y_val = encode_onehot(Y_val, 5)\n",
    "\n",
    "        # Choose appropriate learning rate and no. of epoch\n",
    "        epochs = 40\n",
    "        learning_rate = 0.01\n",
    "        batch_size = 8\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Change training loop as you see fit\n",
    "    for epoch in range(epochs):\n",
    "        Y_pred_train = np.zeros_like(Y_train)\n",
    "        Y_pred_val = np.zeros_like(Y_val)\n",
    "        error = 0\n",
    "        for batch_start in range(0, len(x_train) - 1, batch_size):\n",
    "            x = x_train[batch_start:batch_start+batch_size]\n",
    "            y = y_train[batch_start:batch_start+batch_size]\n",
    "\n",
    "            # forward\n",
    "            output = x\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "\n",
    "            # error (display purpose only)\n",
    "            error += cross_entropy(y, output) * len(x)\n",
    "            Y_pred_train[batch_start:batch_start+batch_size] = decode_onehot(output)\n",
    "\n",
    "            # backward\n",
    "            output_error = cross_entropy_prime(y, output)\n",
    "\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "\n",
    "        output = x_val\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "\n",
    "        error_val = cross_entropy(y_val, output)\n",
    "        Y_pred_val = decode_onehot(output)\n",
    "\n",
    "        print('{}/{}, Train error={:.8f}, Val error={:.8f}, Train acc={:.4f}, Val acc={:.4f}'.format(\n",
    "            epoch + 1, epochs, error, error_val,\n",
    "            100 * np.sum(Y_pred_train == Y_train) / len(Y_train),\n",
    "            100 * np.sum(Y_pred_val == Y_val) / len(Y_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "dataset = \"mnist\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "train_validate(train_mnist[0], train_mnist[1], \"mnist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "dataset = \"flowers\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_flowers = pkl.load(file)\n",
    "    print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "train_validate(train_flowers[0], train_flowers[1], \"flowers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
